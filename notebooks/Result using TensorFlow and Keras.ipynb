{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46195894",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ba4cc",
   "metadata": {},
   "source": [
    "# PART 1: XOR NETWORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55348bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture:\n",
    "# Dense(2→4) → Tanh → Dense(4→1) → Sigmoid\n",
    "\n",
    "def train_xor_keras():\n",
    "    print(\"\\n=== XOR (Keras) ===\")\n",
    "\n",
    "    X = np.array([[0.,0.],\n",
    "                  [0.,1.],\n",
    "                  [1.,0.],\n",
    "                  [1.,1.]], dtype=np.float32)\n",
    "\n",
    "    Y = np.array([[0.],\n",
    "                  [1.],\n",
    "                  [1.],\n",
    "                  [0.]], dtype=np.float32)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            4,\n",
    "            input_shape=(2,),\n",
    "            activation=\"tanh\",\n",
    "            kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    model.fit(X, Y, epochs=10000, verbose=0)\n",
    "    gradient_check_xor(model, X, Y)\n",
    "    preds = model.predict(X)\n",
    "    print(\"XOR raw predictions:\")\n",
    "    print(preds)\n",
    "    print(\"XOR rounded predictions:\")\n",
    "    print((preds > 0.5).astype(int))\n",
    "\n",
    "\n",
    "\n",
    "def gradient_check_xor(model, X, Y, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Numerical gradient checking for the XOR network.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Gradient Checking (XOR) ===\")\n",
    "\n",
    "    # Use MSE loss\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # Pick ONE weight tensor to test (first layer kernel)\n",
    "    W = model.trainable_variables[0]\n",
    "\n",
    "    # Compute analytical gradient using TensorFlow\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(Y, preds)\n",
    "    grad_analytical = tape.gradient(loss, W)\n",
    "\n",
    "    # Convert to numpy for numerical computation\n",
    "    W_np = W.numpy()\n",
    "    grad_numerical = np.zeros_like(W_np)\n",
    "\n",
    "    # Compute numerical gradients\n",
    "    for i in range(W_np.shape[0]):\n",
    "        for j in range(W_np.shape[1]):\n",
    "            original_value = W_np[i, j]\n",
    "\n",
    "            # W + epsilon\n",
    "            W_np[i, j] = original_value + epsilon\n",
    "            W.assign(W_np)\n",
    "            loss_plus = loss_fn(Y, model(X)).numpy()\n",
    "\n",
    "            # W - epsilon\n",
    "            W_np[i, j] = original_value - epsilon\n",
    "            W.assign(W_np)\n",
    "            loss_minus = loss_fn(Y, model(X)).numpy()\n",
    "\n",
    "            # Numerical gradient\n",
    "            grad_numerical[i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "            # Restore original value\n",
    "            W_np[i, j] = original_value\n",
    "\n",
    "    # Restore original weights\n",
    "    W.assign(W_np)\n",
    "\n",
    "    # Compare gradients\n",
    "    diff = np.linalg.norm(grad_analytical.numpy() - grad_numerical)\n",
    "    norm = np.linalg.norm(grad_analytical.numpy()) + np.linalg.norm(grad_numerical)\n",
    "\n",
    "    relative_error = diff / norm\n",
    "\n",
    "    print(\"Analytical gradient:\\n\", grad_analytical.numpy())\n",
    "    print(\"Numerical gradient:\\n\", grad_numerical)\n",
    "    print(\"Relative error:\", relative_error)\n",
    "\n",
    "    if relative_error < 1e-1:\n",
    "        print(\"Gradient check PASSED\")\n",
    "    else:\n",
    "        print(\"Gradient check FAILED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d579300",
   "metadata": {},
   "source": [
    "# PART 2: AUTOENCODER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture:\n",
    "# Encoder: 784→512→128→latent\n",
    "# Decoder: latent→128→512→784\n",
    "\n",
    "def load_mnist_flat():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_test = x_test[:5000]\n",
    "    y_test = y_test[:5000]\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "    x_test  = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_autoencoder(latent_dim=64):\n",
    "\n",
    "    encoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            512,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(784,),\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            128,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(latent_dim)\n",
    "    ])\n",
    "\n",
    "    decoder = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            128,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(latent_dim,),\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            512,\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            784,\n",
    "            activation=\"sigmoid\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "\n",
    "def train_autoencoder():\n",
    "    print(\"\\n=== Autoencoder (Keras) ===\")\n",
    "\n",
    "    x_train, x_test, y_train, y_test = load_mnist_flat()\n",
    "    encoder, decoder, autoencoder = build_autoencoder(latent_dim=64)\n",
    "\n",
    "    autoencoder.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "\n",
    "    history = autoencoder.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        epochs=200,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, x_test)\n",
    "    )\n",
    "\n",
    "    return encoder, autoencoder, x_train, x_test, y_train, y_test, history\n",
    "\n",
    "def train_svm_on_latent(encoder, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train an SVM on the latent features extracted from the autoencoder.\n",
    "    Evaluate test accuracy, confusion matrix, and classification metrics.\n",
    "    \"\"\"\n",
    "    # Extract latent features\n",
    "    latent_train = encoder.predict(X_train)\n",
    "    latent_test = encoder.predict(X_test)\n",
    "\n",
    "    # Train SVM (linear kernel for simplicity)\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(latent_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = svm.predict(latent_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "\n",
    "    print(\"\\n=== SVM on Autoencoder Latent Features ===\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f9b3e",
   "metadata": {},
   "source": [
    "# VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_loss(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Autoencoder Training Loss (Keras)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_reconstruction(autoencoder, X_test, n=5):\n",
    "    preds = autoencoder.predict(X_test[:n])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(n):\n",
    "        plt.subplot(2, n, i+1)\n",
    "        plt.imshow(X_test[i].reshape(28,28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Original\")\n",
    "\n",
    "        plt.subplot(2, n, i+1+n)\n",
    "        plt.imshow(preds[i].reshape(28,28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Reconstructed\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f571b4b",
   "metadata": {},
   "source": [
    "# MAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_xor_keras()\n",
    "    encoder, autoencoder, X_train, X_test, y_train, y_test, history = train_autoencoder()\n",
    "    plot_loss(history)\n",
    "    visualize_reconstruction(autoencoder, X_test)\n",
    "    train_svm_on_latent(encoder, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
