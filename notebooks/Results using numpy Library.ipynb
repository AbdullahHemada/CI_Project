{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427751fd",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2edb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9fda7",
   "metadata": {},
   "source": [
    "# Base Layer Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5965bab",
   "metadata": {},
   "source": [
    "# Dense Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd36af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)  # small weights\n",
    "        self.b = np.zeros((1, output_dim))                     # zero bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x                                             # save input\n",
    "        return x @ self.W + self.b                             # Wx + b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # grad = dL/dZ (incoming gradient)\n",
    "\n",
    "        # Compute gradients\n",
    "        self.dW = self.x.T @ grad                              # dL/dW\n",
    "        self.db = np.sum(grad, axis=0, keepdims=True)          # dL/db\n",
    "\n",
    "        # Return gradient for next layer (dL/dX)\n",
    "        return grad @ self.W.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c7c912",
   "metadata": {},
   "source": [
    "# Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab76365",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation: Tanh\n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (1 - self.out ** 2)   # tanh derivative\n",
    "\n",
    "\n",
    "# Activation: Sigmoid\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (self.out * (1 - self.out))   # sigmoid derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85274892",
   "metadata": {},
   "source": [
    "# Loss: Mean Squared Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42143e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def mse_grad(y_true, y_pred):\n",
    "    return (2 / (y_true.shape[0] * y_true.shape[1])) * (y_pred - y_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf8474",
   "metadata": {},
   "source": [
    "# Optimizer: SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, layers):\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                layer.W -= self.lr * layer.dW\n",
    "                layer.b -= self.lr * layer.db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c6bb1",
   "metadata": {},
   "source": [
    "# Model Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for l in reversed(self.layers):\n",
    "            grad = l.backward(grad)\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self._cache = None\n",
    "    def forward(self, X):\n",
    "        self._cache = X\n",
    "        return np.maximum(0, X)\n",
    "    def backward(self, d_out):\n",
    "        X = self._cache\n",
    "        dX = d_out * (X > 0).astype(float)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae8ca0",
   "metadata": {},
   "source": [
    "# Part 1: XOR Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7289c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor():\n",
    "    X = np.array([[0.,0.],\n",
    "                  [0.,1.],\n",
    "                  [1.,0.],\n",
    "                  [1.,1.]])\n",
    "    Y = np.array([[0.],[1.],[1.],[0.]])\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(2, 4),\n",
    "        Tanh(),\n",
    "        Dense(4, 1),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "\n",
    "    opt = SGD(lr=0.1)\n",
    "\n",
    "    epochs = 100000\n",
    "    for ep in range(1, epochs+1):\n",
    "        y_pred = model.forward(X)\n",
    "        loss = mse_loss(Y, y_pred)\n",
    "        d_pred = mse_grad(Y, y_pred)\n",
    "        model.backward(d_pred)\n",
    "        opt.step(model.layers)\n",
    "        if ep % 10000 == 0:\n",
    "            print(f\"XOR Epoch {ep}/{epochs} - loss: {loss:.6f}\")\n",
    "\n",
    "    preds = model.forward(X)\n",
    "    print(\"XOR final predictions (raw):\")\n",
    "    print(preds)\n",
    "    print(\"XOR final predictions (rounded):\")\n",
    "    print((preds > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c95c26",
   "metadata": {},
   "source": [
    "# Part 2: Autoencoder + SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MNIST loader\n",
    "def load_mnist_flat():\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    x_test  = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def build_autoencoder(latent_dim=32):\n",
    "    encoder = Sequential([\n",
    "        Dense(784, 512),\n",
    "        ReLU(),\n",
    "        Dense(512, 128),\n",
    "        ReLU(),\n",
    "        Dense(128, latent_dim),\n",
    "\n",
    "    ])\n",
    "\n",
    "    decoder = Sequential([\n",
    "        Dense(latent_dim, 128),\n",
    "        ReLU(),\n",
    "        Dense(128, 512),\n",
    "        ReLU(),\n",
    "        Dense(512, 784),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "\n",
    "    autoencoder = Sequential(encoder.layers + decoder.layers)\n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "\n",
    "def train_autoencoder(epochs=5, batch_size=256, lr=0.1, latent_dim=32):\n",
    "    losses = []\n",
    "    X_train, y_train, X_test, y_test = load_mnist_flat()\n",
    "    encoder, decoder, autoencoder = build_autoencoder(latent_dim)\n",
    "\n",
    "    opt = SGD(lr=lr)\n",
    "    N = X_train.shape[0]\n",
    "    steps = max(1, N // batch_size)\n",
    "\n",
    "    print(\"\\nTraining autoencoder:\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        perm = np.random.permutation(N)\n",
    "        X_train = X_train[perm]\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for step in range(steps):\n",
    "            start = step * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "            xb = X_train[start:end]\n",
    "\n",
    "            y_pred = autoencoder.forward(xb)\n",
    "\n",
    "            loss = mse_loss(xb, y_pred)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            grad = mse_grad(xb, y_pred)\n",
    "            autoencoder.backward(grad)\n",
    "\n",
    "            opt.step(autoencoder.layers)\n",
    "\n",
    "        epoch_loss /= steps\n",
    "        print(f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.6f}\")\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "    return encoder, decoder, autoencoder, X_test, y_test, losses\n",
    "\n",
    "\n",
    "def extract_latent_features(encoder, X):\n",
    "    return encoder.forward(X)\n",
    "\n",
    "\n",
    "def train_svm(latent_train, y_train, latent_test, y_test):\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "    clf = SVC(kernel=\"rbf\")\n",
    "    clf.fit(latent_train, y_train)\n",
    "\n",
    "    preds = clf.predict(latent_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    cr = classification_report(y_test, preds, digits=4)\n",
    "\n",
    "    print(\"\\nSVM Results:\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    print(\"Classification report:\\n\", cr)\n",
    "\n",
    "    return acc, cm, cr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c26f8",
   "metadata": {},
   "source": [
    "# Gradient Checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a423de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check():\n",
    "    np.random.seed(0)\n",
    "\n",
    "    X = np.random.randn(5, 3)\n",
    "    Y = np.random.randn(5, 2)\n",
    "\n",
    "    layer = Dense(3, 2)\n",
    "\n",
    "    # Forward\n",
    "    out = layer.forward(X)\n",
    "    loss = mse_loss(Y, out)\n",
    "\n",
    "    # Backward (analytical gradient)\n",
    "    grad_out = mse_grad(Y, out)\n",
    "    layer.backward(grad_out)\n",
    "    grad_analytic = layer.dW.copy()\n",
    "\n",
    "    # Numerical gradient\n",
    "    eps = 1e-5\n",
    "    grad_numeric = np.zeros_like(layer.W)\n",
    "\n",
    "    for i in range(layer.W.shape[0]):\n",
    "        for j in range(layer.W.shape[1]):\n",
    "            W_orig = layer.W[i, j]\n",
    "\n",
    "            layer.W[i, j] = W_orig + eps\n",
    "            loss_plus = mse_loss(Y, layer.forward(X))\n",
    "\n",
    "            layer.W[i, j] = W_orig - eps\n",
    "            loss_minus = mse_loss(Y, layer.forward(X))\n",
    "\n",
    "            grad_numeric[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "            layer.W[i, j] = W_orig  # restore\n",
    "\n",
    "    diff = np.linalg.norm(grad_analytic - grad_numeric)\n",
    "    print(\"\\nGradient Checking:\")\n",
    "    print(\"||Analytical - Numerical|| =\", diff)\n",
    "\n",
    "    if diff < 1e-6:\n",
    "        print(\"Backpropagation is CORRECT\")\n",
    "    else:\n",
    "        print(\"Backpropagation may be incorrect\")\n",
    "\n",
    "def plot_loss(losses):\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Autoencoder Training Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_reconstruction(autoencoder, X_test, n=5):\n",
    "    preds = autoencoder.forward(X_test[:n])\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Original\")\n",
    "\n",
    "        # Reconstructed\n",
    "        plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(preds[i].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Reconstructed\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c567ac4",
   "metadata": {},
   "source": [
    "# Run both parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50226c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Part 1: XOR ===\")\n",
    "    train_xor()\n",
    "    gradient_check()\n",
    "    print(\"\\n=== Part 2: Autoencoder + SVM ===\")\n",
    "    encoder, decoder, autoencoder, X_test, y_test,losses = train_autoencoder(\n",
    "        epochs=200, batch_size=256, lr=0.02, latent_dim=64\n",
    "    )\n",
    "\n",
    "    latent_train = extract_latent_features(encoder, X_test[:5000])\n",
    "    latent_test  = extract_latent_features(encoder, X_test[5000:10000])\n",
    "\n",
    "    y_train_small = y_test[:5000]\n",
    "    y_test_small  = y_test[5000:10000]\n",
    "\n",
    "    train_svm(latent_train, y_train_small, latent_test, y_test_small)\n",
    "    plot_loss(losses)\n",
    "    visualize_reconstruction(autoencoder, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
